<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8" />
	<title>Paper Published</title>
	<link href="https://fonts.googleapis.com/css2?family=Varela+Round&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="styles.css">
</head>
<body>
	<div class="container">
		<div class="nav-wrapper">
			<div class="left-side">
				<div class="nav-link-wrapper">
					<a href="index.html"> Home </a> 
				</div>
				<div class="nav-link-wrapper">
					<a href="about.html"> About me </a> 
				</div>
			</div>
			<div class="right-side">
				<div class="brand">
					<b>SRIRAM MURALIDHARAN</b>
				</div>
			</div>
		</div>
	</div>

	<div class="content-wrapper-pro">
		<center><h1>PAPER: Unsupervised CBIR System using deeplearning and Apache Spark</h1></center>
		<div class="two-column-wrapper">
			<div class="profile-image-wrapper">
				<h2>1.1 Introduction</h2>
				<p>
					Image retrieval searches and retrieves similar images from a database based on a query image. Text Based Image Retrival (TBIR) and Content Based Image Retrieval (CBIR) are the two ways in which similar images can be retrieved. 
				</p>
				<p>
					CBIR plays a major role in security systems to extract the images with similar features. It finds applications in web search engines, industries and architectural designs to retrieve the relevant image to detect crack and to find the images with same texture patterns. Further, it finds applications in medicine, fingerprint identification, digital libraries, biodiversity information systems, digital libraries, crime prevention and historical research.
				</p>
				<p>
					Currently, the amount of digital images grows exponentially due to the increase of online users in the Internet. Considering this, distributed CBIR system has been developed using Hadoop & Map Reduce. The recent research of CBIR is shifted to the use of deep neural networks to extract the features. These works have shown good results on many datasets and outperformed handcrafted features subject to the condition of fine tuning of the deep neural network. 
					CBIR has two parts namely, feature extraction and finding similar images using a suitable distance measure. In the context of feature extraction, if the dataset is labeled, a supervised deep learning neural network can be better used to extract the features. However, if the dataset is large in size and unlabeled, annotation of these images will be time consuming in order to extract the features. In the context of finding similar images, the traditional CBIR system compares the feature vector of the query image with the corresponding feature vector of each and every image in the database. This increases the query response time if the dataset is large in size. Hence, it is necessary to improve the CBIR system with respect to query response time for large dataset. Thus, the objective of this paper is to propose Scalable CBIR system to handle large dataset with improved query response time. 
				</p>

				<h4>2.1.2.Locality Sensitive Hashing: Using Random projection method</h4>
					<img src="images/LSH-flow-chart.png" height="400" width="400" align="right"> 
					Random projection is a technique for representing high-dimensional data in low-dimensional feature space (dimensionality reduction). It gained traction for its ability to approximately preserve relations (pairwise distance or cosine similarity) in low-dimensional space while being computationally less expensive.
					The core idea behind random projection is that if points in a vector space are of sufficiently high dimension, then they may be projected into a suitable lower-dimensional space in a way which approximately preserves the distances between the points.
					Consider a high-dimensional data represented as a matrix D, with n observations (columns of matrix) and d features (rows of the matrix). It can be projected onto a lower dimensional space with k dimensions using a random projection matrix R. Mathematically, the lower dimensional representation P can be obtained as:<br>
					<b><center>[projected]<sub>kxn</sub> =  [random]<sub>kxd</sub> *[origial]<sub>dxn</sub></center></b><br>
					<b>2.1.2.1 Basic algorithm to generate a bitwise hash table:</b><br>
				    1. Create k random vectors of length d each, where k is the size of bitwise hash value and d is the dimension of the feature vector.<br>
				    2. For each random vector, compute the dot product of the random vector and the observation. If the result of the dot product is positive, assign the bit value as 1 else 0<br>
				    3. Concatenate all the bit values computed for k dot products<br>
				    4. Repeat the above two steps for all observations to compute hash values for all observations<br>
				    5. Group observations with same hash values together to create a LSH table<br><br><br><br>

				    <center>fig.3 img_retireval</center>
					<center><img src="images/sample_retrieval.png"></center>
			</div>
			
			<div class="profile-content-wrapper">
				<img src="images/spark.png" height="300" width="400">
				<img src="images/hadoop1.jpg" height="170" width="300">
				<h2>  2.1 Proposed Model</h2>
				<img src="images/ae.png" height="270" width="400" align="right"> 
			
				<h4>2.1.1.Building of Denoising Auto-Encoder</h4>
					However, when there are more nodes in the hidden layer than there are inputs, the network has the risk
					of learning. Denoising Autoencoders solve this problem by corrupting the data on purpose by randomly turning
					some of the input values to zero. Denoising refers to intentionally adding noise to the raw input before providing
					it to the network. It creates a corrupted copy of the input by introducing some noise. This helps to avoid the
					autoencoders to copy the input to the output without learning features about the data. Now the autoencoder
					has to remove the corruption to generate an output that is similar to the input. Output is compared with input
					and not with noised input. To minimize the loss function the process has to continue until the convergence.
					Hence, CBIR utilizes the Denoising auto encoder to extract the features

				<center>fig.1 architecture</center>
				<img src="images/Denoise-AE.png" width="800">
				<center>fig.2 working</center>
				<center><img src="images/Autoencoder-working.png" height="300"></center> 
				
				<h4>2.1.3 K-Nearest Neighbours</h4>
				KNN is a model that classifies data points based on the points that are most similar to it. It uses test data to make an “educated guess” on what an unclassified point should be classified as.KNN is an algorithm that is considered both non-parametric and an example of lazy learning. 
				Non-parametric means that it makes no assumptions. The model is made up entirely from the data given to it rather than assuming its structure is normal.
				Lazy learning means that the algorithm makes no generalizations. This means that there is little training involved when using this method. Because of this, all of the training data is also used in testing when using KNN.<br>
				Pros:<br>
				1.Easy to use.<br>
				2.Quick calculation time.<br>
				3.Does not make assumptions about the data.<br>
				Cons:<br>
				4.Accuracy depends on the quality of the data.<br>
				5.Must find an optimal k value (number of nearest neighbors).<br>
				6.Poor at classifying data points in a boundary where they can be classified one way or another.<br>
			</div>
		</div>
		<h1>Proof of concept: </h1>
		<div class="scrolling-effect">
			<ul>
				  <li> <img src="images/FINAL/comparisons_chart.png" width="600" height="400"> </li>
				  <li> <img src="images/FINAL/image_ret_times.png" width="600" height="400"> </li>
				  <li> <img src="images/FINAL/loss_chart.png" width="600" height="400"> </li>
				  <li> <img src="images/FINAL/model_build_times.png" width="600" height="400"> </li>
			</ul>
		</div>
	</div>
</body>
</html>